{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import librosa\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Conv2D, Flatten, GlobalMaxPooling2D , GlobalMaxPooling1D, MaxPooling2D, MaxPooling1D, Activation, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPool2D, concatenate, Dense, Dropout\n",
    "from keras.layers import  Conv2D, MaxPooling2D, UpSampling2D, Lambda, Reshape\n",
    "from keras.layers import Input, GRU, RepeatVector, BatchNormalization, TimeDistributed, Conv1D\n",
    "from keras.layers import GlobalAveragePooling1D, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from tqdm import tqdm\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "path = \"data/train/audio\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_actual = 'yes no up down left right on off stop go silence unknown'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['right','eight','cat','tree','bed','happy','go','silence',\n",
    "          'dog','no','wow','nine','left','stop','three','sheila','one',\n",
    "          'bird','zero','seven','up','marvin','two',\n",
    "          'house','down','six','yes','on','five','off','four']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wav2mfcc(file,n_fft=960, hop_length=320):\n",
    "    wave, sr = librosa.load(file,sr=None)\n",
    "    # It is very important that all signals be of same length as we need to stack them up vertically on top of \n",
    "    # another for creating a numpy array - eventually for building the model\n",
    "    #print(sr)\n",
    "    if len(wave) > sr:\n",
    "        wave = wave[:sr]\n",
    "    else:\n",
    "        # when the given wave is slightly less than 1 sec, or length is less than 16000, we need to pad with zeros\n",
    "        wave = np.pad(wave, (0,max(0,sr-len(wave))),mode = 'constant')\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=16000,n_mfcc = 12,n_fft=960, hop_length=320)\n",
    "    \n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation Without Data Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split Background noise audio files into chunks of silence files\n",
    "\n",
    "list_files = os.listdir(\"data/train/audio/_background_noise_/\")\n",
    "list_files.remove('.DS_Store')\n",
    "for i,file in enumerate(list_files):\n",
    "    #wav, sr = librosa.load(\"data/train/audio/_background_noise_/\" +file)\n",
    "    wav = AudioSegment.from_wav(\"data/train/audio/_background_noise_/\" +file)\n",
    "    time_steps = 200\n",
    "    sr = 16000\n",
    "    for i in range(len(wav)//time_steps):\n",
    "        chunk = wav[i*time_steps:i*time_steps + sr] \n",
    "        if len(chunk)<200:\n",
    "            continue\n",
    "        chunk = np.array(chunk.get_array_of_samples()).astype(np.int16)\n",
    "        wavfile.write(\"data/train/audio/silence/\"+file.split('.')[0]+str(i)+\".wav\",data=chunk,rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NO AUGMENTATION - Generate numpy files for every label except silence label\n",
    "def preprocessing_data(path=path):\n",
    "    labels_ = labels.copy()\n",
    "    labels_.remove('silence')\n",
    "    for data in labels_:\n",
    "        filepath = path + '/'+data\n",
    "        mfcc_vectors = []\n",
    "        for allfiles in os.listdir(filepath):\n",
    "                 mfcc = wav2mfcc(filepath+ '/' +allfiles)\n",
    "                 #print(mfcc.shape)\n",
    "                 mfcc_vectors.append(mfcc)\n",
    "        mfcc_vectors = np.array(mfcc_vectors)\n",
    "        print(mfcc_vectors.shape)\n",
    "        np.save(\"data_np_mfcc\" + '/' + data + '.npy', mfcc_vectors)\n",
    "        print(data + \".npy  filesaved\")\n",
    "        x = np.load(\"data_np_mfcc\"+ '/' + data + \".npy\")\n",
    "        #print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NO AUGMENTATION - Create MFCCs for silence chunks -\n",
    "mfcc_vectors = []\n",
    "filepath = path + '/' + data\n",
    "print(filepath)\n",
    "for allfiles in os.listdir(filepath):\n",
    "     if allfiles !='.DS_Store':\n",
    "         wave, sr = librosa.load(filepath+ '/' +allfiles, mono=True, sr=None)\n",
    "         if len(wave)>0:\n",
    "             mfcc = wav2mfcc(filepath+ '/' +allfiles)\n",
    "             print(mfcc.shape)\n",
    "             mfcc_vectors.append(mfcc)\n",
    "mfcc_vectors = np.array(mfcc_vectors)\n",
    "np.save(\"data_np_mfcc\" + '/' + data + '.npy', mfcc_vectors)\n",
    "print(data + \".npy  filesaved\")\n",
    "x = np.load(\"data_np_mfcc\"+ '/' + data + \".npy\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation With Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_audio(filepath):\n",
    "    wav,sr = librosa.load(filepath, sr = None)\n",
    "    if len(wav)>sr:\n",
    "        wav = wav[:sr]\n",
    "    else:\n",
    "        wav = np.pad(wav, (0, max(0, sr - len(wav))), \"constant\")\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_audio(filepath):\n",
    "    wav,sr = librosa.load(filepath, sr = None)\n",
    "    if len(wav)>sr:\n",
    "        wav = wav[:sr]\n",
    "    else:\n",
    "        wav = np.pad(wav, (0, max(0, sr - len(wav))), \"constant\")\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_audio(sound, target_dBFS):\n",
    "    change_in_dBFS = target_dBFS - sound.dBFS\n",
    "    return sound.apply_gain(change_in_dBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_audio(audio1, audio2):\n",
    "    joined = (audio1 + audio2)\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stretch(wav, rate=1):\n",
    "    sr = 16000\n",
    "    wav = librosa.effects.time_stretch(wav, rate)\n",
    "    if len(wav)>sr:\n",
    "        wav = wav[:sr]\n",
    "    else:\n",
    "        wav = np.pad(wav, (0, max(0, sr - len(wav))), \"constant\")\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating one big normalized background noise file\n",
    "\n",
    "backgr_noise_files = [file for file in os.listdir('data/train/audio/_background_noise_')]\n",
    "backgr_noise_files.remove('.DS_Store')\n",
    "\n",
    "masterfile_noise = AudioSegment.from_wav(('data/train/audio/_background_noise_/'+backgr_noise_files[0]))\n",
    "masterfile_noise = normalize_audio(noise, -15)\n",
    "\n",
    "for i in range(1, len(backgr_noise_files)):\n",
    "    noise = AudioSegment.from_wav(('data/train/audio/_background_noise_/'+backgr_noise_files[i]))\n",
    "    noise = normalize_audio(noise, -15)\n",
    "    masterfile_noise = join_audio(masterfile_noise, noise) \n",
    "    \n",
    "masterfile_noise_wav = np.array(masterfile_noise.get_array_of_samples()).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating chunks of silence files on normalized background noise :\n",
    "\n",
    "wav = masterfile_noise\n",
    "time_steps = 200\n",
    "sr = 16000\n",
    "for i in range(len(wav)//time_steps):\n",
    "    chunk = wav[i*time_steps:i*time_steps + sr] \n",
    "    if len(chunk)<200:\n",
    "        continue\n",
    "    chunk = np.array(chunk.get_array_of_samples()).astype(np.int16)\n",
    "    wavfile.write(\"data/train/audio/silence/\"+'masterfile_noise'+str(i)+\".wav\",data=chunk,rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## AUGMENTATION - TRAIN DATA NUMPY ARRAYS CREATION\n",
    "def preprocessing_data_augmentation(path=path):\n",
    "    labels_ = labels.copy()\n",
    "    labels_.remove('silence')\n",
    "    for data in labels_:\n",
    "        filepath = path + '/' + data\n",
    "        mfcc_vectors = []\n",
    "        for allfiles in os.listdir(filepath):\n",
    "             mfcc = wav2mfcc(filepath+ '/' +allfiles)\n",
    "             #print(mfcc.shape)\n",
    "             mfcc_vectors.append(mfcc)\n",
    "             wav = read_audio(filepath+ '/' +allfiles)\n",
    "             wav_time_stretch = stretch(wav,rate = 0.8)\n",
    "             mfcc = librosa.feature.mfcc(wav_time_stretch, sr=16000,n_mfcc = 12,n_fft=960, hop_length=320)\n",
    "             #print(mfcc.shape)\n",
    "             mfcc_vectors.append(mfcc)\n",
    "             wav_pitch_shift  = librosa.effects.pitch_shift(wav, sr, n_steps=4)\n",
    "             mfcc = librosa.feature.mfcc(wav_pitch_shift, sr=16000,n_mfcc = 12,n_fft=960, hop_length=320)\n",
    "             #print(mfcc.shape)\n",
    "             mfcc_vectors.append(mfcc)\n",
    "             wav_white_noise = wav + 0.005*(np.random.randn(len(wav)))\n",
    "             mfcc = librosa.feature.mfcc(wav_white_noise, sr=16000,n_mfcc = 12,n_fft=960, hop_length=320)\n",
    "             #print(mfcc.shape)\n",
    "             mfcc_vectors.append(mfcc)\n",
    "        mfcc_vectors = np.array(mfcc_vectors)\n",
    "        \n",
    "        #print(mfcc_vectors.shape)\n",
    "        np.save(\"data_np_mfcc_aug\" + '/' + data + '.npy', mfcc_vectors)\n",
    "        print(data + \".npy  filesaved\")\n",
    "        x = np.load(\"data_np_mfcc_aug\"+ '/' + data + \".npy\")\n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing_data_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA AUGMENTATION - Create MFCCs for silence chunks - silence files\n",
    "data = 'silence'\n",
    "mfcc_vectors = []\n",
    "filepath = path + '/' + data\n",
    "print(filepath)\n",
    "for allfiles in os.listdir(filepath):\n",
    "     if allfiles !='.DS_Store':\n",
    "         print(filepath+ '/' +allfiles)\n",
    "         wave, sr = librosa.load(filepath+ '/' +allfiles, mono=True, sr=None)\n",
    "         if len(wave)>0:\n",
    "             mfcc = wav2mfcc(filepath+ '/' +allfiles)\n",
    "             print(mfcc.shape)\n",
    "             mfcc_vectors.append(mfcc)\n",
    "        \n",
    "mfcc_vectors = np.array(mfcc_vectors)\n",
    "np.save(\"data_np_mfcc_aug\" + '/' + data + '.npy', mfcc_vectors)\n",
    "print(data + \".npy  filesaved\")\n",
    "x = np.load(\"data_np_mfcc_aug\"+ '/' + data + \".npy\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path = \"data/test/audio/\"\n",
    "mfcc_test = []\n",
    "for data in tqdm(os.listdir(test_path)):\n",
    "    mfcc = wav2mfcc(test_path+'/'+data)\n",
    "    mfcc_test.append(mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mfcc_test = np.array(mfcc_test)\n",
    "\n",
    "np.save('mfcc_test.npy', mfcc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_files = [file for file in os.listdir('data/test/audio/')]\n",
    "\n",
    "test_files = np.array(test_files)\n",
    "\n",
    " np.save('test_files.npy', test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
